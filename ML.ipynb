{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n6u-aI2yUE1"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "V2qrOAH3oBRO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Lasso, LassoCV, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-IyB8oyAAJK"
   },
   "source": [
    "# Regression task: California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9hy5IatuD7V-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1011.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>1020.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.562431</td>\n",
       "      <td>35.656882</td>\n",
       "      <td>27.623529</td>\n",
       "      <td>2732.830392</td>\n",
       "      <td>556.577646</td>\n",
       "      <td>1474.960784</td>\n",
       "      <td>515.614706</td>\n",
       "      <td>3.955444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.970947</td>\n",
       "      <td>2.143429</td>\n",
       "      <td>12.311122</td>\n",
       "      <td>2168.037719</td>\n",
       "      <td>423.168029</td>\n",
       "      <td>1116.843167</td>\n",
       "      <td>382.273122</td>\n",
       "      <td>1.957958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.190000</td>\n",
       "      <td>32.560000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.620000</td>\n",
       "      <td>33.910000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1482.000000</td>\n",
       "      <td>301.500000</td>\n",
       "      <td>807.500000</td>\n",
       "      <td>287.750000</td>\n",
       "      <td>2.607425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.530000</td>\n",
       "      <td>34.280000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>2206.500000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>1204.000000</td>\n",
       "      <td>427.000000</td>\n",
       "      <td>3.690750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.037500</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3260.000000</td>\n",
       "      <td>672.500000</td>\n",
       "      <td>1815.750000</td>\n",
       "      <td>626.500000</td>\n",
       "      <td>4.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-115.410000</td>\n",
       "      <td>41.780000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>27700.000000</td>\n",
       "      <td>4386.000000</td>\n",
       "      <td>15037.000000</td>\n",
       "      <td>4072.000000</td>\n",
       "      <td>15.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         longitude     latitude  housing_median_age   total_rooms  \\\n",
       "count  1020.000000  1020.000000         1020.000000   1020.000000   \n",
       "mean   -119.562431    35.656882           27.623529   2732.830392   \n",
       "std       1.970947     2.143429           12.311122   2168.037719   \n",
       "min    -124.190000    32.560000            2.000000     19.000000   \n",
       "25%    -121.620000    33.910000           17.000000   1482.000000   \n",
       "50%    -118.530000    34.280000           28.000000   2206.500000   \n",
       "75%    -118.037500    37.710000           36.000000   3260.000000   \n",
       "max    -115.410000    41.780000           52.000000  27700.000000   \n",
       "\n",
       "       total_bedrooms    population   households  median_income  \n",
       "count     1011.000000   1020.000000  1020.000000    1020.000000  \n",
       "mean       556.577646   1474.960784   515.614706       3.955444  \n",
       "std        423.168029   1116.843167   382.273122       1.957958  \n",
       "min         11.000000     34.000000     9.000000       0.536000  \n",
       "25%        301.500000    807.500000   287.750000       2.607425  \n",
       "50%        452.000000   1204.000000   427.000000       3.690750  \n",
       "75%        672.500000   1815.750000   626.500000       4.856800  \n",
       "max       4386.000000  15037.000000  4072.000000      15.000100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ocean_proximity\n",
       "count             1020\n",
       "unique               4\n",
       "top          <1H OCEAN\n",
       "freq               466"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create pandas dataframe from dataset\n",
    "trainData = pd.read_csv('housing_coursework_train.csv')\n",
    "testData = pd.read_csv('housing_coursework_test.csv')\n",
    "frames = [trainData, testData]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "# Separate features from target variable\n",
    "features=['longitude',\t'latitude',\t'housing_median_age',\t'total_rooms',\t'total_bedrooms',\n",
    "          'population',\t'households',\t'median_income', 'ocean_proximity']\n",
    "X_raw = data[features]\n",
    "y_raw = data['median_house_value']\n",
    "\n",
    "# Describe numerical and categorical columns\n",
    "display(X_raw.select_dtypes(include=np.number).describe())\n",
    "display(X_raw.select_dtypes(exclude=np.number).describe())\n",
    "\n",
    "# Drop unnessecary columns\n",
    "# X_raw = X_raw.drop(columns=['No.']) # Remove the No. column using '.drop' (no longer included in features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EYl4ZxioeFG-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "14     -122.24     37.72                   5        18634          2885.0   \n",
      "34     -122.01     37.57                  14        16199          2993.0   \n",
      "35     -122.05     37.57                   7        10648          1818.0   \n",
      "39     -121.92     37.72                   4         7477          1576.0   \n",
      "47     -120.97     38.42                  16         1748           322.0   \n",
      "..         ...       ...                 ...          ...             ...   \n",
      "199    -119.17     34.16                  17         5276          1020.0   \n",
      "201    -119.04     34.24                  20         7794          1192.0   \n",
      "203    -119.06     34.22                  13         4175          1321.0   \n",
      "207    -118.69     34.18                  11         1177           138.0   \n",
      "208    -118.83     34.33                   6         6679          1164.0   \n",
      "\n",
      "     population  households  median_income ocean_proximity  \n",
      "14         7427        2718         7.6110        NEAR BAY  \n",
      "34         8117        2847         5.8322        NEAR BAY  \n",
      "35         6075        1797         6.1047        NEAR BAY  \n",
      "39         2937        1506         5.1437       <1H OCEAN  \n",
      "47         4930         287         4.3029          INLAND  \n",
      "..          ...         ...            ...             ...  \n",
      "199        4066         984         4.5828      NEAR OCEAN  \n",
      "201        4169        1188         5.9316       <1H OCEAN  \n",
      "203        2257        1271         3.1446       <1H OCEAN  \n",
      "207         415         119        10.0472       <1H OCEAN  \n",
      "208        3196        1157         5.4493       <1H OCEAN  \n",
      "\n",
      "[107 rows x 9 columns]\n",
      "['NEAR BAY' '<1H OCEAN' 'INLAND' 'NEAR OCEAN']\n",
      "['NEAR BAY' '<1H OCEAN' 'INLAND' 'NEAR OCEAN']\n"
     ]
    }
   ],
   "source": [
    "# Print numerical outliers\n",
    "print(X_raw[(X_raw['total_rooms'] < 0) | (X_raw['total_rooms'] > 7500) |\n",
    "            (X_raw['total_bedrooms'] < 0) | (X_raw['total_bedrooms'] > 1250) |\n",
    "            (X_raw['population'] < 0) | (X_raw['population'] > 3000) | \n",
    "            (X_raw['households'] < 0) | (X_raw['households'] > 2000) | \n",
    "            (X_raw['median_income'] < 0) | (X_raw['median_income'] > 10)])\n",
    "\n",
    "# Remove numerical outliers\n",
    "# X_train_raw.loc[X_train_raw.total_rooms>1000,'total_rooms'] = np.nan  # Set total_rooms over 3500 to be NaN\n",
    "# X_train_raw.loc[X_train_raw.total_bedrooms>900,'total_bedrooms'] = np.nan  # Set total_bedrooms over 900 to be NaN\n",
    "\n",
    "# Print categorical outliers\n",
    "print(pd.unique(X_raw['ocean_proximity']))\n",
    "print(pd.unique(X_raw['ocean_proximity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OevtEyfGiR2G"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'sparse_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20316\\2176100527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Feature encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_categorical_imp\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Fit and transform categorical data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mX_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_categorical_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'sparse_output'"
     ]
    }
   ],
   "source": [
    "# Split data into seperate numerical and categorical data\n",
    "X_numerical = X_raw.select_dtypes(include=np.number)\n",
    "X_categorical = X_raw.select_dtypes(exclude=np.number)\n",
    "\n",
    "# Create our imputer objects\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit the imputers on the testing data and transform column\n",
    "numeric_imputer.fit(X_numerical)\n",
    "categorical_imputer.fit(X_categorical)\n",
    "X_numerical_imp = numeric_imputer.transform(X_numerical)  \n",
    "X_categorical_imp = categorical_imputer.transform(X_categorical)\n",
    "\n",
    "# Feature scaling \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_numerical_imp)  # Fit and transform numeric data\n",
    "X_num_sca = scaler.transform(X_numerical_imp)\n",
    "\n",
    "# Feature encoder\n",
    "encoder = OneHotEncoder(handle_unknown ='ignore', sparse_output=False)\n",
    "encoder.fit(X_categorical_imp)  # Fit and transform categorical data\n",
    "X_onehot = encoder.transform(X_categorical_imp)\n",
    "\n",
    "# Concatenate scalled data\n",
    "X = np.concatenate([X_num_sca, X_onehot], axis=1)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-0PeG86z5bhA"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20316\\3939563283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Split the data into non-test/test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_validate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into non-test/test data\n",
    "X_train, X_test_validate, y_train, y_test_validate = train_test_split(X, y_raw, test_size=0.20, shuffle=True, random_state=0)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test_validate, y_test_validate, test_size=0.50, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yRzD84FaNy-o"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20316\\879114049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Linear regression [USE VALIDATION]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlinear_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Create object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlinear_y_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Make predictions using the testing set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plt.scatter(y_test, linear_y_pred)  # Scatter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Linear regression [USE VALIDATION]\n",
    "linear_obj = sklearn.linear_model.LinearRegression(fit_intercept=True)  # Create object\n",
    "linear_obj.fit(X_train, y_train)  # Train the model using the training sets\n",
    "linear_y_pred = linear_obj.predict(X_test)  # Make predictions using the testing set\n",
    "# plt.scatter(y_test, linear_y_pred)  # Scatter\n",
    "print(linear_obj.score(X_train, y_train))\n",
    "print(linear_obj.score(X_validate, y_validate))\n",
    "\n",
    "# Fine tuning\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Lasso\n",
    "# lasso_obj = Lasso(alpha=1.0)  \n",
    "# # lasso_obj.fit(X_train, y_train)\n",
    "# lasso_obj = LassoCV(alphas = [0.0001, 0.001,0.01, 0.1, 1, 10]).fit(X_train, y_train)\n",
    "# lasso_y_pred= lasso_obj.predict(X_test)  \n",
    "# # plt.scatter(y_test, lasso_y_pred)\n",
    "# print(lasso_obj.score(X_train, y_train))\n",
    "# print(lasso_obj.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6qlMsUbUj7Q"
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error(MSE)\n",
    "print(\"Linear MSE\",mean_squared_error(y_test, linear_y_pred))\n",
    "print(\"Lasso MSE\",mean_squared_error(y_test, lasso_y_pred))\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "print(\"Linear MAE\",mean_absolute_error(y_test,linear_y_pred))\n",
    "print(\"Lasso MAE\",mean_absolute_error(y_test,lasso_y_pred))\n",
    "\n",
    "# Root Mean Squared Error(RMSE)\n",
    "print(\"Linear RMSE\",np.sqrt(mean_squared_error(y_test,linear_y_pred)))\n",
    "print(\"Lasso RMSE\",np.sqrt(mean_squared_error(y_test,lasso_y_pred)))\n",
    "\n",
    "# # Root Mean Squared Log Error(RMSLE)\n",
    "# print(\"Linear RMSE\",np.log(np.sqrt(mean_squared_error(y_test,linear_y_pred))))\n",
    "# print(\"Lasso RMSE\",np.log(np.sqrt(mean_squared_error(y_test,lasso_y_pred))))\n",
    "\n",
    "# R Squared (R2)\n",
    "print(\"Linear R2\",r2_score(y_test,linear_y_pred))\n",
    "print(\"Lasso R2\",r2_score(y_test,lasso_y_pred))\n",
    "\n",
    "# Adjusted R Squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImfztaMXKqRh"
   },
   "source": [
    "Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSQyyH_F666i"
   },
   "outputs": [],
   "source": [
    "# Data visualisation\n",
    "sns.pairplot(data=testData, diag_kind='kde')\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(testData[['longitude',\t'latitude',\t'housing_median_age',\t'total_rooms',\t'total_bedrooms',\t'population',\t'households',\t'median_income', 'median_house_value']].corr(), cmap='Blues', annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaIZ1yRfBeeJ"
   },
   "outputs": [],
   "source": [
    "# Feature selection (Do not implement)\n",
    "X_raw = SelectKBest(f_regression, k = 3).fit_transform(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X64x1muc4mTT"
   },
   "source": [
    "# Classification task: Titanic dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "den42oKk4YH8"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "trainData = pd.read_csv('/content/sample_data/Titanic_train.csv')\n",
    "testData = pd.read_csv('/content/sample_data/Titanic_test.csv')\n",
    "frames = [trainData, testData]\n",
    "data = pd.concat(frames)\n",
    "display(data)\n",
    "\n",
    "# Allocate features and target column\n",
    "features=['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']\n",
    "X_raw = data[features]\n",
    "y_raw = data['Target: Survived']  # Target variable\n",
    "# X_raw = X_raw.drop(columns=['PassengerId']) # Remove the PassengerId column using '.drop'\n",
    "\n",
    "display(X_raw.head())\n",
    "print(X_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VI3y9Z6sYQsP"
   },
   "outputs": [],
   "source": [
    "# Identify outliers\n",
    "display(X_raw.select_dtypes(include=np.number).describe()) # display numeric colmuns\n",
    "display(X_raw.select_dtypes(exclude=np.number).describe()) # describe numeric columns for obvious outliers (missing ticket number and embarked location)\n",
    "\n",
    "# Look at the numerical columns and see if there are any obvious outliers - no outliers found in numerical columns (besides NaN)\n",
    "display(X_raw.select_dtypes(include=np.number).describe())\n",
    "print(X_raw[(X_raw['Pclass'] < 1) | (X_raw['Pclass'] > 3) | \n",
    "            (X_raw['Age'] < 0) | (X_raw['Age'] > 100) | \n",
    "            (X_raw['SibSp'] < 0) | (X_raw['SibSp'] > 10) |\n",
    "            (X_raw['Parch'] < 0) | (X_raw['Parch'] > 10) |\n",
    "            (X_raw['Fare'] < 0) | (X_raw['Fare'] > 250)])\n",
    "\n",
    "# Overwrite outliers - none (besides NaN)\n",
    "\n",
    "# Print unique values\n",
    "print(pd.unique(trainData['Sex']))\n",
    "print(pd.unique(trainData['Embarked']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-Ibt8gbhTwU"
   },
   "outputs": [],
   "source": [
    "# Split data into seperate numerical and categorical data\n",
    "X_numerical = X_raw.select_dtypes(include=np.number)\n",
    "X_categorical = X_raw.select_dtypes(exclude=np.number)\n",
    "\n",
    "# Create our imputer objects\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Fit the imputers on the testing data and transform column\n",
    "numeric_imputer.fit(X_numerical)\n",
    "categorical_imputer.fit(X_categorical)\n",
    "X_numerical_imp = numeric_imputer.transform(X_numerical)  \n",
    "X_categorical_imp = categorical_imputer.transform(X_categorical)\n",
    "\n",
    "# Feature scaling \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_numerical_imp)  # Fit and transform numeric data\n",
    "X_num_sca = scaler.transform(X_numerical_imp)\n",
    "\n",
    "# Feature encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoder.fit(X_categorical_imp)  # Fit and transform categorical data\n",
    "X_onehot = encoder.transform(X_categorical_imp)\n",
    "\n",
    "# Concatenate scalled data\n",
    "X = np.concatenate([X_num_sca, X_onehot], axis=1)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B98HBhy43i0X"
   },
   "outputs": [],
   "source": [
    "# Split the data into non-test/test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_raw, test_size=0.20, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbibQYd_3o56"
   },
   "outputs": [],
   "source": [
    "# Implement classification models (ligistic & SVM)\n",
    "logistic_obj = LogisticRegression()\n",
    "logistic_obj.fit(X_train, y_train)\n",
    "logistic_pred = logistic_obj.predict(X_test)\n",
    "# Use score method to get accuracy of model\n",
    "score = logistic_obj.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "# SVM\n",
    "SVM_obj = SVC(kernel='linear')\n",
    "SVM_obj.fit(X_train, y_train)\n",
    "SVM_pred = SVM_obj.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, SVM_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmfjxvI6MCMW"
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_pred)\n",
    "logistic_precision = precision_score(y_test, logistic_pred)\n",
    "logistic_recall = recall_score(y_test, logistic_pred)\n",
    "logistic_f1score = f1_score(y_test, logistic_pred)\n",
    "\n",
    "SVM_accuracy = accuracy_score(y_test, SVM_pred)\n",
    "SVM_precision = precision_score(y_test, SVM_pred)\n",
    "SVM_recall = recall_score(y_test, SVM_pred)\n",
    "SVM_f1score = f1_score(y_test, SVM_pred)\n",
    "\n",
    "print(f\"Logistic Accuracy = {logistic_accuracy.round(4)}\")\n",
    "print(f\"SVM Accuracy = {SVM_accuracy.round(4)}\")\n",
    "\n",
    "print(f\"Logistic Precision = {logistic_precision.round(4)}\")\n",
    "print(f\"SVM Precision = {SVM_precision.round(4)}\")\n",
    "\n",
    "print(f\"Logistic Recall = {logistic_recall.round(4)}\")\n",
    "print(f\"SVM Recall = {SVM_recall.round(4)}\")\n",
    "\n",
    "print(f\"Logistic F1 Score = {logistic_f1score.round(4)}\")\n",
    "print(f\"SVM F1 Score = {SVM_f1score.round(4)}\")\n",
    "\n",
    "# Generate confusion matrix for the predictions\n",
    "logistic_conf_matrix = confusion_matrix(y_test, logistic_pred)\n",
    "print(\"Logistic Confsion Matrix:\")\n",
    "logistic_conf_matrix\n",
    "\n",
    "SVM_conf_matrix = confusion_matrix(y_test, SVM_pred)\n",
    "print(\"SVM Confsion Matrix:\")\n",
    "SVM_conf_matrix"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2QQrOE6sSVFCfDhtwdduR",
   "collapsed_sections": [
    "X64x1muc4mTT"
   ],
   "mount_file_id": "1C8gelcvMffK9IUjsEuMTj7QBkjLdzL0E",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
